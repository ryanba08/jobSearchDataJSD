{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import time\n",
    "from bs4 import BeautifulSoup\n",
    "import csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#if you use two words for job type need to input like this Data-Analyst have to use uppercase on first letter as well even if only one word\n",
    "def extract(jobtype,page=1):\n",
    "    headers = {'User-Agent': 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/125.0.0.0 Safari/537.36'}\n",
    "    r = requests.get(url=f'https://jobcube.com/{jobtype}-jobs?f=68&pg={page}',headers=headers)\n",
    "    if r.status_code == 200:\n",
    "        print('Success',page)\n",
    "        return r.content\n",
    "    else:\n",
    "        print(f'status code {r.status_code} sorry :( failed on page: {page}')\n",
    "    return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#extract job info from job cards and return a list of dictionarys with the title description salary and date posted\n",
    "def pull_data(soup):\n",
    "    data = []\n",
    "    job_cards = soup.find_all('div',class_='p-job')\n",
    "    for card in job_cards:\n",
    "        job_info = {'title':'','description':'','salary':'','date_posted':''}\n",
    "        company_name = card.find('p',class_=\"p-job_companyName\")\n",
    "        if company_name:\n",
    "            job_info['title'] = company_name.text\n",
    "        job_info['description'] = card.find('h2').text.strip()\n",
    "        job_info['salary'] = card.find('li',class_=\"p-job_salary\").text.strip()\n",
    "        job_info['date_posted'] = card.find('p',class_=\"p-job_date\").text\n",
    "        \n",
    "        data.append(job_info)\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Success 1\n",
      "Success 2\n",
      "Success 3\n",
      "Success 4\n",
      "Success 5\n",
      "Success 6\n",
      "Success 7\n",
      "Success 8\n",
      "Success 9\n",
      "Success 10\n",
      "Success 11\n",
      "Success 12\n",
      "Success 13\n",
      "Success 14\n",
      "Success 15\n",
      "Success 16\n",
      "Success 17\n",
      "Success 18\n",
      "Success 19\n",
      "Success 20\n",
      "Success 21\n",
      "Success 22\n",
      "Success 23\n",
      "Success 24\n",
      "Success 25\n",
      "Success 26\n",
      "Success 27\n",
      "Success 28\n",
      "Success 29\n",
      "Success 30\n",
      "Success 31\n",
      "Success 32\n",
      "Success 33\n",
      "Success 34\n",
      "Success 35\n",
      "Success 36\n",
      "Success 37\n",
      "Success 38\n",
      "Success 39\n",
      "Success 40\n",
      "Success 41\n",
      "Success 42\n",
      "Success 43\n",
      "Success 44\n",
      "Success 45\n",
      "Success 46\n",
      "Success 47\n",
      "Success 48\n",
      "Success 49\n",
      "Success 50\n",
      "Success 51\n",
      "Success 52\n",
      "Success 53\n",
      "Success 54\n",
      "Success 55\n",
      "Success 56\n",
      "Success 57\n",
      "Success 58\n",
      "Success 59\n",
      "Success 60\n"
     ]
    }
   ],
   "source": [
    "#max page number unable to find website only has next arrows come up with future solution. for now 60 pages is a good place to start. website throws 404 status code when you reached past max page number.\n",
    "data = []\n",
    "jobtype = 'Data'\n",
    "for i in range(1,61):\n",
    "    html = extract(jobtype=jobtype,page=i)\n",
    "    if html:\n",
    "        soup = BeautifulSoup(html, 'html.parser')\n",
    "        data.append(pull_data(soup))\n",
    "    time.sleep(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#get rid of duplicates and merge lists to one\n",
    "finalData = []\n",
    "for page in data:\n",
    "    for item in page:\n",
    "        if item not in finalData:\n",
    "            finalData.append(item)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "headers = finalData[0].keys()\n",
    "with open('jobcubeData.csv', 'w', newline='') as csv_file:\n",
    "    writer = csv.DictWriter(csv_file, fieldnames=headers)\n",
    "    writer.writeheader()\n",
    "    writer.writerows(finalData)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
